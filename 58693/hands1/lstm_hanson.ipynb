{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this hands on you will build a model that once trained on a peice of text data can generate its own sequnce of words in a similar fashion as in trained data.\n",
    "\n",
    "- Follow the instructions provided for each cell and and code accordingly. \n",
    "- In order to run the cell press shift+enter.\n",
    "- make sure you have run all the cells before submitting the hands on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the below cell to import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the text data from story.txt file and split the text into seperate tokens, assign thr array of tokens to variable *training_data*\n",
    "\n",
    "### Expected output:\n",
    "    array(['long', 'ago', ',', 'the', 'mice', 'had', 'a', 'general',\n",
    "       'council', 'to']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['long', 'ago', ',', 'the', 'mice', 'had', 'a', 'general', 'council', 'to']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Start code here\n",
    "\n",
    "from tokenize import tokenize\n",
    "file_content = open(\"story.txt\").read()\n",
    "file_content\n",
    "training_data = file_content.split()\n",
    "###End code\n",
    "training_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the unique tokens in training_data nas sort them alphabetical order and assign the sorted list to variable words\n",
    "### create dictionary ind_to_word to map index to word\n",
    "### create another dictionary word_to_ind to reverse map word to their respective index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words:  [',', '.', '?', 'a', 'about', 'ago', 'agree', 'all', 'always', 'an'] \n",
      "\n",
      "index_to_words:  [(0, ','), (1, '.'), (2, '?'), (3, 'a'), (4, 'about'), (5, 'ago'), (6, 'agree'), (7, 'all'), (8, 'always'), (9, 'an')] \n",
      "\n",
      "word_to_index:  [('said', 75), ('consider', 25), ('who', 106), ('?', 2), ('council', 28), ('neck', 58), ('was', 99), ('take', 83), ('ribbon', 73), ('impossible', 42)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "####Start code here\n",
    "words = list(sorted(set(training_data)))\n",
    "ind_to_word = {}\n",
    "for i,word in enumerate(words):\n",
    "    ind_to_word[i]=word\n",
    "word_to_ind = {}\n",
    "for i,word in enumerate(words):\n",
    "    word_to_ind[word]=i\n",
    "###End code\n",
    "print(\"words: \", words[:10], \"\\n\")\n",
    "print(\"index_to_words: \", list(ind_to_word.items())[:10], \"\\n\")\n",
    "print(\"word_to_index: \", list(word_to_ind.items())[:10], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write a function to generate training dataset \n",
    "    - parameters: dataset: orginal dataset\n",
    "                  look_back: the window size that tells the number of previous values in the series to look for to                   predict the next one.\n",
    "    - returns: feature and target arrays\n",
    "    \n",
    "example: \n",
    "         for window size 1:\n",
    "         dataset = [1,2,3,4]  \n",
    "         feature = [[1],[2],[3]]    \n",
    "         target = [2,3,4]  \n",
    "         \n",
    "         for window size 2:\n",
    "         feature = [[1,2],[2,3]]  \n",
    "         target = [3,4]  \n",
    "### Expected output when you when you call generate_dataset on training_data and look_back = 10 :\n",
    "input:  [[48, 5, 0, 85, 56, 37, 3, 35, 28, 92], [5, 0, 85, 56, 37, 3, 35, 28, 92, 25], [0, 85, 56, 37, 3, 35, 28, 92, 25, 102]]  \n",
    "labels:  [25, 102, 53]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  [[ 48   5   0  85  56  37   3  35  28  92]\n",
      " [  5   0  85  56  37   3  35  28  92  25]\n",
      " [  0  85  56  37   3  35  28  92  25 102]]\n",
      "labels:  [ 25 102  53]\n"
     ]
    }
   ],
   "source": [
    "####Start code here\n",
    "def generate_dataset(dataset, look_back=10):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        x=i+look_back\n",
    "#         print(x)\n",
    "        a = dataset[i:x]\n",
    "        v=[]\n",
    "        for k in a:\n",
    "            v.append(word_to_ind[k])\n",
    "        dataX.append(v)\n",
    "        dataY.append(word_to_ind[dataset[x]])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "    \n",
    "    \n",
    "###End code\n",
    "\n",
    "inputs, labels = generate_dataset(training_data, 10)\n",
    "print(\"input: \", inputs[:3])\n",
    "print(\"labels: \", labels[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next step is to  reshape the inputs and normalize them.\n",
    "    - This step is coded for you\n",
    "    - Run the below cell to prepare the data for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 10\n",
    "X_modified = np.reshape(inputs, (len(inputs), look_back, 1))\n",
    "\n",
    "X_modified = X_modified / float(len(words))\n",
    "Y_modified = np_utils.to_categorical(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using keras Sequential() class create a model having two LSTM block and one fully connected  layer with activation softmax\n",
    "### apply dropout with probability p = 0.2 between the layers of LSTM\n",
    "### compile the model with categorical_crossentropy loss and  adam optimizer\n",
    "\n",
    "### Expected output\n",
    "<img src=\"lstm.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_10 (LSTM)               (None, 10, 400)           643200    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 10, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 112)               44912     \n",
      "=================================================================\n",
      "Total params: 1,969,712\n",
      "Trainable params: 1,969,712\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(51)\n",
    "###Start code here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(400, return_sequences=True,input_shape=(X_modified.shape[1], X_modified.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(400, input_shape=(X_modified.shape[1], X_modified.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(Y_modified.shape[1], activation='softmax'))\n",
    "\n",
    "\n",
    "###End code\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model.fit() on train data with features as X_modified and target as Y_modified for 50 epoches and batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "193/193 [==============================] - 5s 24ms/step - loss: 4.6863\n",
      "Epoch 2/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 4.5825\n",
      "Epoch 3/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 4.5025\n",
      "Epoch 4/50\n",
      "193/193 [==============================] - 3s 16ms/step - loss: 4.4666\n",
      "Epoch 5/50\n",
      "193/193 [==============================] - 3s 16ms/step - loss: 4.4524\n",
      "Epoch 6/50\n",
      "193/193 [==============================] - 3s 16ms/step - loss: 4.4792\n",
      "Epoch 7/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 4.4459\n",
      "Epoch 8/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 4.4505\n",
      "Epoch 9/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 4.4319\n",
      "Epoch 10/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 4.4472\n",
      "Epoch 11/50\n",
      "193/193 [==============================] - 3s 16ms/step - loss: 4.4281\n",
      "Epoch 12/50\n",
      "193/193 [==============================] - 3s 18ms/step - loss: 4.4249\n",
      "Epoch 13/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 4.4373\n",
      "Epoch 14/50\n",
      "193/193 [==============================] - 3s 16ms/step - loss: 4.4433\n",
      "Epoch 15/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 4.4174\n",
      "Epoch 16/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 4.4411\n",
      "Epoch 17/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 4.4267\n",
      "Epoch 18/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 4.4088\n",
      "Epoch 19/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 4.4137\n",
      "Epoch 20/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 4.4258\n",
      "Epoch 21/50\n",
      "193/193 [==============================] - 3s 18ms/step - loss: 4.4015\n",
      "Epoch 22/50\n",
      "193/193 [==============================] - 3s 18ms/step - loss: 4.3304\n",
      "Epoch 23/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 4.2690\n",
      "Epoch 24/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 4.2403\n",
      "Epoch 25/50\n",
      "193/193 [==============================] - 3s 16ms/step - loss: 4.1204\n",
      "Epoch 26/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 4.0259\n",
      "Epoch 27/50\n",
      "193/193 [==============================] - 3s 18ms/step - loss: 3.8759\n",
      "Epoch 28/50\n",
      "193/193 [==============================] - 4s 18ms/step - loss: 3.8562\n",
      "Epoch 29/50\n",
      "193/193 [==============================] - 3s 18ms/step - loss: 3.6530\n",
      "Epoch 30/50\n",
      "193/193 [==============================] - 3s 18ms/step - loss: 3.5417\n",
      "Epoch 31/50\n",
      "193/193 [==============================] - 3s 18ms/step - loss: 3.4826\n",
      "Epoch 32/50\n",
      "193/193 [==============================] - 3s 18ms/step - loss: 3.2132\n",
      "Epoch 33/50\n",
      "193/193 [==============================] - 3s 18ms/step - loss: 3.1231\n",
      "Epoch 34/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 2.9302\n",
      "Epoch 35/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 2.9274\n",
      "Epoch 36/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 2.7444\n",
      "Epoch 37/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 2.6367\n",
      "Epoch 38/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 2.4841\n",
      "Epoch 39/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 2.3077\n",
      "Epoch 40/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 2.2378\n",
      "Epoch 41/50\n",
      "193/193 [==============================] - 3s 18ms/step - loss: 2.1986\n",
      "Epoch 42/50\n",
      "193/193 [==============================] - 3s 18ms/step - loss: 2.0568\n",
      "Epoch 43/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 2.0420\n",
      "Epoch 44/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 2.0473\n",
      "Epoch 45/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 1.8403\n",
      "Epoch 46/50\n",
      "193/193 [==============================] - 3s 18ms/step - loss: 1.7546\n",
      "Epoch 47/50\n",
      "193/193 [==============================] - 3s 18ms/step - loss: 1.5920\n",
      "Epoch 48/50\n",
      "193/193 [==============================] - 3s 18ms/step - loss: 1.4372\n",
      "Epoch 49/50\n",
      "193/193 [==============================] - 3s 17ms/step - loss: 1.3649\n",
      "Epoch 50/50\n",
      "193/193 [==============================] - 3s 16ms/step - loss: 1.2861\n"
     ]
    }
   ],
   "source": [
    "###Start code here\n",
    "train_logs = model.fit(X_modified, Y_modified, epochs=50,batch_size=10) \n",
    "###End code\n",
    "with open(\"output.txt\", \"w+\") as file:\n",
    "    file.write(\"train score {0:.2f}\\n\".format(train_logs.history[\"loss\"][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below codes takes a random sequence of words and generates more sequnce using the model you trained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " which he thought would meet the case . you will all agree , said he , that our chief danger consists what the sly and treacherous know approach , now agree , up another , and neck what venture old what some when neighbourhood . will agree met met met , said up that treacherous a is the an you old with she another the , their the proposal with we ? the you the will met met all , the proposal met with she was in take . this all agree agree , to up receive round , in neighbourhood . mice old , from a . spoke old\n"
     ]
    }
   ],
   "source": [
    "string_mapped = inputs[50].copy()\n",
    "# print(string_mapped)\n",
    "full_string = [ind_to_word[value] for value in string_mapped]\n",
    "#generating characters\n",
    "for i in range(100):\n",
    "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
    "    x = x / float(len(words))\n",
    "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
    "    seq = [ind_to_word[value] for value in string_mapped]\n",
    "    full_string.append(ind_to_word[pred_index])\n",
    "    string_mapped=list(string_mapped)\n",
    "#     print(string_mapped,pred_index)\n",
    "#     np.append(pred_index,string_mapped)\n",
    "    string_mapped.append(pred_index)\n",
    "    string_mapped=np.array(string_mapped)\n",
    "    string_mapped = string_mapped[1:len(string_mapped)]\n",
    "\n",
    "    \n",
    "txt=\"\"\n",
    "for word in full_string:\n",
    "    txt = txt+\" \"+word\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
